\doxysection{RMSProp Class Reference}
\hypertarget{classRMSProp}{}\label{classRMSProp}\index{RMSProp@{RMSProp}}


\doxylink{classRMSProp}{RMSProp} optimiser.  




{\ttfamily \#include $<$optimiser.\+hpp$>$}

Inheritance diagram for RMSProp\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{classRMSProp}
\end{center}
\end{figure}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classRMSProp_ad68b4478e7bb40d4dc28b4b21fae0a24}{RMSProp}} (\mbox{\hyperlink{classNeuralNetwork}{Neural\+Network}} \&nn, float beta, float epsilon, float learning\+\_\+rate)
\begin{DoxyCompactList}\small\item\em Constructor for \doxylink{classRMSProp}{RMSProp} optimiser. \end{DoxyCompactList}\item 
void \mbox{\hyperlink{classRMSProp_ac38b540eb9a7530720098847c29fcb75}{update\+\_\+weights}} (const \mbox{\hyperlink{classGradients}{Gradients}} \&grads, \mbox{\hyperlink{classNeuralNetwork}{Neural\+Network}} \&nn) override
\begin{DoxyCompactList}\small\item\em Update the weights and biases of the neural network using the gradients. \end{DoxyCompactList}\item 
\Hypertarget{classRMSProp_a2562687b36bdde83839c2bfbe650d9b8}\label{classRMSProp_a2562687b36bdde83839c2bfbe650d9b8} 
{\bfseries \texorpdfstring{$\sim$}{\string~}\+RMSProp} ()
\begin{DoxyCompactList}\small\item\em Virtual destructor. \end{DoxyCompactList}\item 
float \mbox{\hyperlink{classRMSProp_a895b06ad5747b629eb361b8cce028362}{get\+\_\+learning\+\_\+rate}} () const override
\begin{DoxyCompactList}\small\item\em Get the learning rate of the optimiser. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsection*{Public Member Functions inherited from \mbox{\hyperlink{classIOptimiser}{IOptimiser}}}
\begin{DoxyCompactItemize}
\item 
virtual {\bfseries \texorpdfstring{$\sim$}{\string~}\+IOptimiser} ()
\begin{DoxyCompactList}\small\item\em Virtual destructor. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\doxylink{classRMSProp}{RMSProp} optimiser. 

This class implements the \doxylink{classRMSProp}{RMSProp} optimisation algorithm for updating the weights and biases of a neural network. It uses an adaptive learning rate based on the moving average of the squared gradients. 

\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classRMSProp_ad68b4478e7bb40d4dc28b4b21fae0a24}\index{RMSProp@{RMSProp}!RMSProp@{RMSProp}}
\index{RMSProp@{RMSProp}!RMSProp@{RMSProp}}
\doxysubsubsection{\texorpdfstring{RMSProp()}{RMSProp()}}
{\footnotesize\ttfamily \label{classRMSProp_ad68b4478e7bb40d4dc28b4b21fae0a24} 
RMSProp\+::\+RMSProp (\begin{DoxyParamCaption}\item[{\mbox{\hyperlink{classNeuralNetwork}{Neural\+Network}} \&}]{nn}{, }\item[{float}]{beta}{, }\item[{float}]{epsilon}{, }\item[{float}]{learning\+\_\+rate}{}\end{DoxyParamCaption})}



Constructor for \doxylink{classRMSProp}{RMSProp} optimiser. 


\begin{DoxyParams}{Parameters}
{\em nn} & Neural network getting trained \\
\hline
{\em beta} & Exponential decay rate for the moving average \\
\hline
{\em epsilon} & Small constant to prevent division by zero \\
\hline
{\em learning\+\_\+rate} & Learning rate for the optimiser \\
\hline
\end{DoxyParams}


\doxysubsection{Member Function Documentation}
\Hypertarget{classRMSProp_a895b06ad5747b629eb361b8cce028362}\index{RMSProp@{RMSProp}!get\_learning\_rate@{get\_learning\_rate}}
\index{get\_learning\_rate@{get\_learning\_rate}!RMSProp@{RMSProp}}
\doxysubsubsection{\texorpdfstring{get\_learning\_rate()}{get\_learning\_rate()}}
{\footnotesize\ttfamily \label{classRMSProp_a895b06ad5747b629eb361b8cce028362} 
float RMSProp\+::get\+\_\+learning\+\_\+rate (\begin{DoxyParamCaption}{}{}\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [override]}, {\ttfamily [virtual]}}



Get the learning rate of the optimiser. 

\begin{DoxyReturn}{Returns}
Learning rate 
\end{DoxyReturn}


Implements \mbox{\hyperlink{classIOptimiser_a705f5a50c7f09582bc7a0988baa75f93}{IOptimiser}}.

\Hypertarget{classRMSProp_ac38b540eb9a7530720098847c29fcb75}\index{RMSProp@{RMSProp}!update\_weights@{update\_weights}}
\index{update\_weights@{update\_weights}!RMSProp@{RMSProp}}
\doxysubsubsection{\texorpdfstring{update\_weights()}{update\_weights()}}
{\footnotesize\ttfamily \label{classRMSProp_ac38b540eb9a7530720098847c29fcb75} 
void RMSProp\+::update\+\_\+weights (\begin{DoxyParamCaption}\item[{const \mbox{\hyperlink{classGradients}{Gradients}} \&}]{grads}{, }\item[{\mbox{\hyperlink{classNeuralNetwork}{Neural\+Network}} \&}]{nn}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [override]}, {\ttfamily [virtual]}}



Update the weights and biases of the neural network using the gradients. 


\begin{DoxyParams}{Parameters}
{\em grads} & \doxylink{classGradients}{Gradients} for weights and biases \\
\hline
{\em nn} & Neural network to update \\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classIOptimiser_a01a3fa5fd46b6bc1d997aa6aa775f7ec}{IOptimiser}}.



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
include/neural\+\_\+network/\mbox{\hyperlink{optimiser_8hpp}{optimiser.\+hpp}}\item 
src/neural\+\_\+network/\mbox{\hyperlink{optimiser_8cpp}{optimiser.\+cpp}}\end{DoxyCompactItemize}
